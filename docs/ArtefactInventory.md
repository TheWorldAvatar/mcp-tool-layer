### Artefact inventory + promotion checklist

This document lists the project’s key **generated artefacts**, their **locations**, and whether they are:
- **Generated (candidate)**: written under `ai_generated_contents_candidate/` by generation scripts.
- **Consumed (production)**: read by the runtime pipeline from `ai_generated_contents/` (default behavior).
- **Promoted**: should be copied from candidate → production when you “release” a new set of prompts/specs.

It also calls out important **exceptions** and the **MCP-config-driven** nature of “scripts”.

---

### 1) Ontology build artefacts (generation outputs)

#### 1.1 Iteration specifications (iterations.json)
- **Candidate (generated)**:
  - `ai_generated_contents_candidate/iterations/<ontology>/iterations.json`
- **Production (consumed by default)**:
  - `ai_generated_contents/iterations/<ontology>/iterations.json`
- **Generated by**:
  - `python -m src.agents.scripts_and_prompts_generation.iteration_creation_agent ...`
  - or one-shot `python -m src.agents.scripts_and_prompts_generation.generation_main ...`
- **Consumed by**:
  - main ontology extraction: `src/pipelines/main_ontology_extractions/extract.py` (production only)
  - main KG building: `src/pipelines/main_kg_building/build.py` (production only)
  - extensions KG building: `src/pipelines/extensions_kg_building/build.py` (**candidate-first**, then production fallback)

#### 1.2 Prompts (markdown)

##### Extraction prompts
- **Candidate (generated)**:
  - `ai_generated_contents_candidate/prompts/<ontology>/EXTRACTION_ITER_*.md`
  - `ai_generated_contents_candidate/prompts/<ontology>/PRE_EXTRACTION_ITER_*.md` (if used)
- **Production (consumed by default)**:
  - `ai_generated_contents/prompts/<ontology>/EXTRACTION_ITER_*.md`
  - `ai_generated_contents/prompts/<ontology>/PRE_EXTRACTION_ITER_*.md`
- **Generated by**:
  - `python -m src.agents.scripts_and_prompts_generation.task_extraction_prompt_creation_agent ...`
  - or one-shot `generation_main.py`
- **Consumed by**:
  - `src/pipelines/top_entity_extraction/extract.py` (uses `ai_generated_contents/prompts/.../EXTRACTION_ITER_1.md`)
  - `src/pipelines/main_ontology_extractions/extract.py`
  - `src/pipelines/extensions_extractions/extract.py` (often uses candidate iterations to discover prompt paths)

##### KG-building prompts
- **Candidate (generated)**:
  - `ai_generated_contents_candidate/prompts/<ontology>/KG_BUILDING_ITER_*.md`
  - `ai_generated_contents_candidate/prompts/<extension>/EXTENSION.md` (and related templates if generated)
- **Production (consumed by default)**:
  - `ai_generated_contents/prompts/<ontology>/KG_BUILDING_ITER_*.md`
  - `ai_generated_contents/prompts/<extension>/EXTENSION.md`
- **Generated by**:
  - `python -m src.agents.scripts_and_prompts_generation.task_prompt_creation_agent ...`
  - or one-shot `generation_main.py`
- **Consumed by**:
  - `src/pipelines/top_entity_kg_building/build.py` (iter1 KG prompt)
  - `src/pipelines/main_kg_building/build.py` (iter2/3/4 KG prompts referenced by `iterations.json`)
  - `src/pipelines/extensions_kg_building/build.py` (extension prompts; has candidate fallback logic)

#### 1.3 Ontology MCP scripts (generated code)

These are the LLM-generated MCP server implementations for ontology KG building / tooling.

- **Candidate (generated)**:
  - `ai_generated_contents_candidate/scripts/<ontology>/<ontology>_creation.py` (underlying “Script”)
  - `ai_generated_contents_candidate/scripts/<ontology>/main.py` (FastMCP server wrapper)
- **Production**:
  - There is **no universal “production scripts directory”** automatically used by the pipeline.
  - Whether a runtime run uses candidate scripts depends on **which MCP config JSON** you point it at.
- **Generated by**:
  - `python -m src.agents.scripts_and_prompts_generation.mcp_underlying_script_creation_agent ...`
  - `python -m src.agents.scripts_and_prompts_generation.mcp_main_script_creation_agent ...`
  - or one-shot `generation_main.py`
- **Consumed by** (indirectly):
  - `models/BaseAgent.py` reads MCP server definitions from `models/MCPConfig.py` which loads `configs/<mcp_set_name>.json`.

---

### 2) SPARQL artefacts (top-entity parsing)

#### 2.1 Top entity parsing query
- **Production (generated directly; NOT candidate)**:
  - `ai_generated_contents/sparqls/<ontology>/top_entity_parsing.sparql`
- **Generated by**:
  - `python -m src.agents.scripts_and_prompts_generation.top_entity_sparql_generation_agent ...`
- **Consumed by**:
  - `src/pipelines/top_entity_kg_building/build.py` (parses `iteration_1.ttl` → `mcp_run/iter1_top_entities.json`)

**Exception note**: this generator writes to `ai_generated_contents/` directly. There is no candidate equivalent unless you add one.

---

### 3) MCP configuration artefacts (what actually controls “which scripts are used”)

MCP server execution is controlled by config JSON files in `configs/`.

Common ones:
- **Local tool-layer MCPs**: `configs/mcp_configs.json` (user-local paths; copied from `.example`)
- **Chemistry MCPs** (external tools): `configs/chemistry.json` (e.g., pubchem / ccdc / enhanced_websearch / chemistry)
- **Extensions MCPs**: `configs/extension.json` (e.g., mops_extension / ontospecies_extension / ccdc)
- **LLM-created MCP**: `configs/run_created_mcp.json` (often points to `sandbox.code.mcp_creation.main` + optional websearch)

Generation pipeline output:
- **Generated MCP config JSON**:
  - `configs/generated_ontology_mcps.json`
  - Produced by `generation_main.py` as a registry for candidate scripts.

Generic runner special case:
- `generic_main.py --test` writes `configs/test_mcp_config.json` that points to:
  - `-m ai_generated_contents_candidate.scripts.<ontology>.main`

---

### 4) Runtime pipeline artefacts (per DOI hash)

These live under `data/<hash>/...` and are produced by `generic_main.py` + `src/pipelines/*` steps.

Core outputs (high-level):
- `data/<hash>/mcp_run/iter1_top_entities.json`
- `data/<hash>/mcp_run/iter*_hints_*.txt`
- `data/<hash>/iteration_1.ttl`
- `data/<hash>/intermediate_ttl_files/iteration_*_*.ttl`
- `data/<hash>/ontomops_output/*.ttl`
- `data/<hash>/ontospecies_output/*.ttl`

Evaluation merge outputs:
- `evaluation/data/merged_tll/<hash>/<hash>.ttl` (and `link.ttl`, plus JSON summaries)

---

### 5) Grounding artefacts (generation + runtime)

Grounding documentation: `src/agents/grounding/README.md`.

Key artefacts:
- **Label cache** (built from target KG endpoint):
  - `data/grounding_cache/<ontology>/labels/**/*.jsonl`
- **Script C** (query + fuzzy lookup module; current example path):
  - `sandbox/script_c_query_client.py`
- **Grounding MCP config**:
  - `configs/grounding.json`
- **Grounded TTL outputs** (runtime, optional):
  - `<input>_grounded.ttl` next to the input TTL (or specified path)

---

### 6) Promotion checklist (candidate → production)

This is the practical “release” step so the default runtime pipeline (which reads `ai_generated_contents/...`) uses your newly generated artefacts.

#### 6.1 Must-promote for main ontology runs (recommended)
- **Iterations**:
  - `ai_generated_contents_candidate/iterations/ontosynthesis/iterations.json`
  → `ai_generated_contents/iterations/ontosynthesis/iterations.json`
- **Prompts**:
  - `ai_generated_contents_candidate/prompts/ontosynthesis/*.md`
  → `ai_generated_contents/prompts/ontosynthesis/*.md`

Why: main pipeline steps (`top_entity_extraction`, `main_ontology_extractions`, `main_kg_building`) read from `ai_generated_contents/...` by default.

#### 6.2 Extensions (optional, because runtime is candidate-first)
- `ai_generated_contents_candidate/iterations/ontomops/iterations.json`
  → `ai_generated_contents/iterations/ontomops/iterations.json`
- `ai_generated_contents_candidate/iterations/ontospecies/iterations.json`
  → `ai_generated_contents/iterations/ontospecies/iterations.json`
- `ai_generated_contents_candidate/prompts/ontomops/*.md`
  → `ai_generated_contents/prompts/ontomops/*.md`
- `ai_generated_contents_candidate/prompts/ontospecies/*.md`
  → `ai_generated_contents/prompts/ontospecies/*.md`

Why optional: `extensions_kg_building` explicitly checks candidate first, then production.

#### 6.3 SPARQL (no promotion step today)
- `top_entity_parsing.sparql` is generated directly into:
  - `ai_generated_contents/sparqls/<ontology>/top_entity_parsing.sparql`

#### 6.4 MCP scripts (promotion is “by config”, not by directory)
There is no single required copy step for MCP scripts; instead you choose one of:
- **Run using candidate scripts**: point MCP config to `ai_generated_contents_candidate.scripts.<ontology>.main` (e.g., `generic_main.py --test`).
- **Run using stable scripts**: copy or vendor scripts somewhere stable and update the relevant `configs/*.json` entry to point there.

---

### 7) Truth table (what’s true, with exceptions)

- **(1) Prompts/iterations/scripts generated in candidate?**  
  - **Yes for prompts + iterations + MCP scripts** (default generators write to `ai_generated_contents_candidate/`).
  - **Exception**: top-entity parsing SPARQL writes directly to `ai_generated_contents/`.

- **(2) Runtime pipeline uses production by default?**  
  - **Main ontology steps**: yes, they read `ai_generated_contents/...`.
  - **Extensions**: mixed; `extensions_kg_building` is **candidate-first**, then production fallback.
  - **MCP scripts**: selected by MCP config JSON; not inherently tied to `ai_generated_contents/`.

- **(3) Recommended workflow: generate → review → promote to production?**  
  - **Yes**, with the above exceptions and the “MCP scripts are config-driven” nuance.

